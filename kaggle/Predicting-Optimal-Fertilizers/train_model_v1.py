import os
from collections import Counter

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from matplotlib import pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from torch.utils.tensorboard import SummaryWriter
from xgboost import XGBClassifier

from data_preprocessing import generate_data_report

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(current_dir, 'data')

writer = SummaryWriter(log_dir=os.path.join(current_dir, 'runs'))

# åœ¨è®­ç»ƒå¼€å§‹å‰å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_file_path = os.path.join(current_dir, 'best_log.txt')
# è®¾ç½®è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ å½“å‰è®¾å¤‡: {device}")


class FertilizerDataset(torch.utils.data.Dataset):
    def __init__(self, features, labels):
        self.X = features
        self.y = labels

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)


class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        super(MLP, self).__init__()
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)


def prepare_data():
    '''
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    :return:
    '''
    file_path = os.path.join(data_dir, 'train.csv')
    df = pd.read_csv(file_path)

    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name', 'id'])
    y = df['Fertilizer Name'].values
    '''
    äººå·¥æ ¡éªŒæ•°æ®
    '''
    # æ£€æŸ¥åˆ†ç±»æ˜¯å¦å‡è¡¡
    print(Counter(y))

    print("åŸå§‹åˆ—å:", X.columns.tolist())

    # å®šä¹‰é¢„å¤„ç†å™¨ï¼šç±»åˆ«å‹åˆ—åš OneHotï¼Œæ•°å€¼å‹åˆ—æ ‡å‡†åŒ–
    # æ˜ç¡®æŒ‡å®šç±»åˆ«å‹å’Œæ•°å€¼å‹åˆ—  Temparature,Humidity,Moisture,Soil Type,Crop Type,Nitrogen,Potassium,Phosphorous
    categorical_cols = ['Soil Type', 'Crop Type']
    numerical_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']  # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ•°å€¼åˆ—

    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', StandardScaler(), numerical_cols)
    ])

    X_processed = preprocessor.fit_transform(X)

    # æ£€æŸ¥OneHot ç¼–ç åçš„ç¨€ç–çŸ©é˜µæ˜¯å¦è¢«æ­£ç¡®è½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µï¼Ÿ
    # ç‰¹å¾ä¸­æ˜¯å¦æœ‰å¤§é‡ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ï¼Ÿ
    print("X_processed type:", type(X_processed))
    print("X_processed shape:", X_processed.shape)
    print("X_processed sample:\n", X_processed[:5])

    # ç¼–ç æ ‡ç­¾ï¼ˆè™½ç„¶ä½ å·²å¤„ç†è¿‡ï¼Œä½†ç¡®ä¿æ˜¯æ•´æ•°å½¢å¼ï¼‰
    le = LabelEncoder()
    y = le.fit_transform(y)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)

    # æ„å»º Dataset å’Œ DataLoader
    train_dataset = FertilizerDataset(X_train, y_train)
    val_dataset = FertilizerDataset(X_val, y_val)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64)

    return train_loader, val_loader, len(le.classes_)


def train_torch_model():
    train_loader, val_loader, num_classes = prepare_data()
    input_dim = train_loader.dataset.X.shape[1]  # è‡ªåŠ¨è·å–ç‰¹å¾æ•°é‡
    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´ input_dim
    # æ‰¾ä¸€ä¸ªé€‚åˆå¤šç±»åˆ«åˆ†ç±»çš„æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    # å®šä¹‰ä¼˜åŒ–å™¨
    # optimizer = optim.Adam(model.parameters(), lr=0.0005)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)
    # è®­ç»ƒå¾ªç¯
    epochs = 50
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch")
    # åˆå§‹åŒ–æœ€ä½³å‡†ç¡®ç‡
    best_acc = 0.0

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 'min', patience=3)

    for epoch in range(epochs):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä»¥ä¾¿å¯ç”¨dropoutã€batch normalizationç­‰åœ¨è®­ç»ƒæ—¶éœ€è¦çš„ç‰¹æ€§
        model.train()
        # åˆå§‹åŒ–æ€»æŸå¤±ä¸º0ï¼Œç”¨äºç´¯è®¡æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±å€¼
        total_loss = 0
        # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œè·å–æ¯ä¸ªæ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾
        for X_batch, y_batch in train_loader:
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆå¦‚GPUï¼‰ä¸Š
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            # æ¸…é›¶ä¼˜åŒ–å™¨çš„æ¢¯åº¦ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯å½±å“ä¸‹ä¸€æ¬¡è®¡ç®—
            optimizer.zero_grad()
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œè·å–æ¨¡å‹çš„è¾“å‡ºç»“æœ
            outputs = model(X_batch)
            # è®¡ç®—æ¨¡å‹è¾“å‡ºç»“æœä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„æŸå¤±å€¼
            loss = criterion(outputs, y_batch)
            # å¯¹æŸå¤±å€¼è¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹å‚æ•°çš„æ¢¯åº¦
            loss.backward()
            # æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¸€æ¬¡æ›´æ–°
            optimizer.step()
            # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„æŸå¤±å€¼åˆ°æ€»æŸå¤±ä¸­ï¼Œç”¨äºåç»­çš„æŸå¤±å€¼è·Ÿè¸ªå’Œå¯è§†åŒ–
            total_loss += loss.item()

        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä»¥ç¦ç”¨dropoutç­‰ä»…åœ¨è®­ç»ƒæœŸé—´å¯ç”¨çš„åŠŸèƒ½
        model.eval()
        # åˆå§‹åŒ–åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        y_true, y_pred = [], []
        # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜æ¶ˆè€—
        with torch.no_grad():
            # éå†éªŒè¯æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
            for X_batch, y_batch in val_loader:
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
                outputs = model(X_batch)
                # é€šè¿‡è·å–æ¯ä¸ªè¾“å‡ºè¡Œä¸­æœ€å¤§å€¼çš„ç´¢å¼•æ¥ç¡®å®šé¢„æµ‹æ ‡ç­¾
                preds = torch.argmax(outputs, dim=1)
                # æ‰“å°ç¬¬ä¸€ä¸ª batch çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹å€¼
                # print("çœŸå®æ ‡ç­¾:", y_batch.cpu().numpy())
                # print("é¢„æµ‹æ ‡ç­¾:", preds.cpu().numpy())
                # å°†çœŸå®æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_trueåˆ—è¡¨ä¸­
                y_true.extend(y_batch.cpu().numpy())
                # å°†é¢„æµ‹æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_predåˆ—è¡¨ä¸­
                y_pred.extend(preds.cpu().numpy())

        # çœ‹å“ªäº›ç±»åˆ«çš„ precision/recall å¾ˆä½ï¼› çœ‹çœ‹æ˜¯å¦æ˜¯å­¦ä¸ä¼šæŸä¸€ç±»è¿˜æ˜¯éƒ½å­¦ä¸ä¼š
        print(classification_report(y_true, y_pred))
        # è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡
        acc = accuracy_score(y_true, y_pred)
        # æ‰“å°å½“å‰è½®æ¬¡çš„è®­ç»ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬è½®æ¬¡ã€æŸå¤±å€¼å’ŒéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss:.4f}, Val Acc: {acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_torch_model.pth'))
            print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼Œåªä¿ç•™æœ€æ–°ä¸€è¡Œï¼‰
            with open(log_file_path, 'w') as log_file:
                log_file.write(f"Best Accuracy: {best_acc:.4f}\n")

        scheduler.step(total_loss)
        writer.add_scalar('Loss/train', total_loss, epoch)
        writer.add_scalar('Accuracy/val', acc, epoch)

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return model


def tradition_model():
    '''
    ä¸ºäº†åˆ¤æ–­æ˜¯å¦æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿæµ‹è¯•ä¸€ä¸ªä¼ ç»Ÿåˆ†ç±»å™¨ï¼ˆå¦‚ RandomForestClassifierï¼‰ï¼š
    å¦‚æœä¼ ç»Ÿæ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œåˆ™è¯´æ˜æ•°æ®æœ¬èº«æ˜¯å¯å­¦çš„ï¼Œé—®é¢˜å‡ºåœ¨ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®¾è®¡æˆ–è®­ç»ƒä¸Šã€‚
    :return:
    '''

    file_path = os.path.join(data_dir, 'train.csv')
    df = pd.read_csv(file_path)
    df = add_agricultural_features(df)
    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name', 'id','Soil Type'])
    y = df['Fertilizer Name'].values
    print(df.describe())
    # å®šä¹‰é¢„å¤„ç†å™¨ï¼šç±»åˆ«å‹åˆ—åš OneHotï¼Œæ•°å€¼å‹åˆ—æ ‡å‡†åŒ–
    # æ˜ç¡®æŒ‡å®šç±»åˆ«å‹å’Œæ•°å€¼å‹åˆ—  Temparature,Humidity,Moisture,Soil Type,Crop Type,Nitrogen,Potassium,Phosphorous
    # categorical_cols = ['Soil Type', 'Crop Type']
    categorical_cols = [ 'Crop Type','Sugarcane_Clayey']
    # columns_to_exclude = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']
    columns_to_exclude = []
    # Humidityã€ Nitrogen Phosphorous Nâ€”â€”sqrtå½±å“ä¸å¤§
    # Moisture NPK æ¯”è¾ƒå°
    numerical_cols = [col for col in X.columns if col not in categorical_cols and col not in columns_to_exclude]
    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', StandardScaler(), numerical_cols)
    ])

    X_processed = preprocessor.fit_transform(X)

    # generate_data_report(df, target_col='Fertilizer Name')

    # ç¼–ç æ ‡ç­¾ï¼ˆè™½ç„¶ä½ å·²å¤„ç†è¿‡ï¼Œä½†ç¡®ä¿æ˜¯æ•´æ•°å½¢å¼ï¼‰
    le = LabelEncoder()
    y = le.fit_transform(y)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)

    # clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42) 17.3%
    # XGBoost ç¤ºä¾‹
    # åˆå§‹åŒ–XGBClassifieræ¨¡å‹ï¼Œé…ç½®ç‰¹å®šçš„å‚æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
    print("XGBoost æ¨¡å‹è®­ç»ƒä¸­...")
    clf = XGBClassifier(
        n_estimators=500,
        learning_rate=0.1,
        max_depth=5,
        min_child_weight=3,
        gamma=0.1,
        subsample=0.8,
        colsample_bytree=0.7,
        eval_metric='mlogloss',
        use_label_encoder=False,
        tree_method='hist'
    )
    eval_set = [(X_val, y_val)]
    clf.fit(X_train, y_train, early_stopping_rounds=20, eval_set=eval_set, verbose=False)
    print("XGBoostè®­ç»ƒç»“æŸ")
    # # LightGBM ç¤ºä¾‹
    # clf = LGBMClassifier(
    #     n_estimators=1000,
    #     learning_rate=0.05,
    #     max_depth=6,
    #     subsample=0.8,
    #     colsample_bytree=0.8,
    #     random_state=42
    # )
    y_pred = clf.predict(X_val)
    print("Val Accuracy:", accuracy_score(y_val, y_pred))
    print(classification_report(y_val, y_pred))
    import seaborn as sns

    sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# -----------------------------
# ç‰¹å¾æ„é€ 
# -----------------------------
def add_agricultural_features(df):
    df['Moisture_Squared'] = df['Moisture'] ** 3
    df['Phosphorous_Squared'] =df['Phosphorous']**2
    df['Nitrogen_Squared'] =df['Nitrogen']**2
    df['Temparature_Squared'] =df['Temparature']**2
    df['Humidity_Squared'] =df['Humidity']**2
    df['NPK_Sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']
    df['N_P_Ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-5)
    df['P_K_Ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-5)
    df['Env_Index'] = df['Temparature'] * df['Humidity'] * df['Moisture']
    df['Crop_Soil_Interaction'] = df['Crop Type'] + '_' + df['Soil Type']
    crop_soil_preference = {
        ('Wheat', 'Clay'): 1.2,
        ('Maize', 'Loam'): 1.3,
        ('Millets', 'Sandy'): 1.5,
    }

    df['Crop_Soil_Preference'] = df.apply(
        lambda row: crop_soil_preference.get((row['Crop Type'], row['Soil Type']), 1.0), axis=1)
    df['Weighted_Nitrogen'] = df['Nitrogen'] * df['Crop_Soil_Preference']
    """
    æ„é€ å†œä¸šé¢†åŸŸç›¸å…³ç‰¹å¾
    """
    # df['NPK_Sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']
    # df['N_P_Ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-5)
    # df['P_K_Ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-5)
    # df['Env_Index'] = df['Temparature'] * df['Humidity'] * df['Moisture'] * 20
    # df['Fertility_Score'] = (
    #         df['Nitrogen'] * 0.3 +
    #         df['Phosphorous'] * 0.3 +
    #         df['Potassium'] * 0.4
    # )
    # TODO ç”¨æ¨¡å‹è¾…åŠ©è‡ªå·±å­¦ä¹ æƒé‡
    # crop_n_preference = {
    #     'Wheat': 0.8,
    #     'Maize': 0.7,
    #     'Oil seeds': 0.3,
    #     'Paddy': 0.5,
    #     'Cotton': 0.6,
    #     'Barley': 0.7,
    #     'Millets': 0.5,
    #     'Sugarcane': 0.4,
    #     'Ground Nuts': 0.4,
    #     'Tobacco': 0.5,
    #     'Pulses': 0.4
    # }
    # df['Crop_Nitrogen_Preference'] = df['Crop Type'].map(crop_n_preference).fillna(0.5)
    # df['Weighted_N'] = df['Nitrogen'] * df['Crop_Nitrogen_Preference']
    # df['N_sqrt'] = np.sqrt(df['Nitrogen'])
    # df['NK_ratio'] = df['Nitrogen'] / (df['Potassium'] + 1e-5)
    return df


if __name__ == '__main__':
    tradition_model()
    # trained_model = train_torch_model()
