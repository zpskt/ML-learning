import os
from collections import Counter

import joblib
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from matplotlib import pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.utils import compute_sample_weight
from torch.utils.tensorboard import SummaryWriter
from xgboost import XGBClassifier

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(current_dir, 'data')

writer = SummaryWriter(log_dir=os.path.join(current_dir, 'runs'))

# åœ¨è®­ç»ƒå¼€å§‹å‰å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_file_path = os.path.join(current_dir, 'best_log.txt')
# è®¾ç½®è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ å½“å‰è®¾å¤‡: {device}")


class FertilizerDataset(torch.utils.data.Dataset):
    def __init__(self, features, labels):
        self.X = features
        self.y = labels

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)


class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        super(MLP, self).__init__()
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)


def prepare_data():
    '''
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    :return:
    '''
    file_path = os.path.join(data_dir, 'train.csv')
    df = pd.read_csv(file_path)
    # æ„å»ºé¢†åŸŸåŒ–ç‰¹å¾
    df = add_agricultural_features(df)
    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name', 'id'])

    y = df['Fertilizer Name'].values
    '''
    äººå·¥æ ¡éªŒæ•°æ®
    '''
    # æ£€æŸ¥åˆ†ç±»æ˜¯å¦å‡è¡¡
    print(Counter(y))

    print("åŸå§‹åˆ—å:", X.columns.tolist())

    # å®šä¹‰é¢„å¤„ç†å™¨ï¼šç±»åˆ«å‹åˆ—åš OneHotï¼Œæ•°å€¼å‹åˆ—æ ‡å‡†åŒ–
    # æ˜ç¡®æŒ‡å®šç±»åˆ«å‹å’Œæ•°å€¼å‹åˆ—  Temparature,Humidity,Moisture,Soil Type,Crop Type,Nitrogen,Potassium,Phosphorous
    categorical_cols = ['Soil Type', 'Crop Type']
    numerical_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']  # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ•°å€¼åˆ—

    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', StandardScaler(), numerical_cols)
    ])

    X_processed = preprocessor.fit_transform(X)

    # æ£€æŸ¥OneHot ç¼–ç åçš„ç¨€ç–çŸ©é˜µæ˜¯å¦è¢«æ­£ç¡®è½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µï¼Ÿ
    # ç‰¹å¾ä¸­æ˜¯å¦æœ‰å¤§é‡ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ï¼Ÿ
    print("X_processed type:", type(X_processed))
    print("X_processed shape:", X_processed.shape)
    print("X_processed sample:\n", X_processed[:5])

    # ç¼–ç æ ‡ç­¾ï¼ˆè™½ç„¶ä½ å·²å¤„ç†è¿‡ï¼Œä½†ç¡®ä¿æ˜¯æ•´æ•°å½¢å¼ï¼‰
    le = LabelEncoder()
    y = le.fit_transform(y)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)

    # æ„å»º Dataset å’Œ DataLoader
    train_dataset = FertilizerDataset(X_train, y_train)
    val_dataset = FertilizerDataset(X_val, y_val)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64)

    return train_loader, val_loader, len(le.classes_)


def train_torch_model():
    train_loader, val_loader, num_classes = prepare_data()
    input_dim = train_loader.dataset.X.shape[1]  # è‡ªåŠ¨è·å–ç‰¹å¾æ•°é‡
    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´ input_dim
    # æ‰¾ä¸€ä¸ªé€‚åˆå¤šç±»åˆ«åˆ†ç±»çš„æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    # å®šä¹‰ä¼˜åŒ–å™¨
    # optimizer = optim.Adam(model.parameters(), lr=0.0005)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)
    # è®­ç»ƒå¾ªç¯
    epochs = 50
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch")
    # åˆå§‹åŒ–æœ€ä½³å‡†ç¡®ç‡
    best_acc = 0.0

    for epoch in range(epochs):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä»¥ä¾¿å¯ç”¨dropoutã€batch normalizationç­‰åœ¨è®­ç»ƒæ—¶éœ€è¦çš„ç‰¹æ€§
        model.train()
        # åˆå§‹åŒ–æ€»æŸå¤±ä¸º0ï¼Œç”¨äºç´¯è®¡æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±å€¼
        total_loss = 0
        # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œè·å–æ¯ä¸ªæ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾
        for X_batch, y_batch in train_loader:
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆå¦‚GPUï¼‰ä¸Š
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            # æ¸…é›¶ä¼˜åŒ–å™¨çš„æ¢¯åº¦ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯å½±å“ä¸‹ä¸€æ¬¡è®¡ç®—
            optimizer.zero_grad()
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œè·å–æ¨¡å‹çš„è¾“å‡ºç»“æœ
            outputs = model(X_batch)
            # è®¡ç®—æ¨¡å‹è¾“å‡ºç»“æœä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„æŸå¤±å€¼
            loss = criterion(outputs, y_batch)
            # å¯¹æŸå¤±å€¼è¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹å‚æ•°çš„æ¢¯åº¦
            loss.backward()
            # æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¸€æ¬¡æ›´æ–°
            optimizer.step()
            # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„æŸå¤±å€¼åˆ°æ€»æŸå¤±ä¸­ï¼Œç”¨äºåç»­çš„æŸå¤±å€¼è·Ÿè¸ªå’Œå¯è§†åŒ–
            total_loss += loss.item()

        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä»¥ç¦ç”¨dropoutç­‰ä»…åœ¨è®­ç»ƒæœŸé—´å¯ç”¨çš„åŠŸèƒ½
        model.eval()
        # åˆå§‹åŒ–åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        y_true, y_pred = [], []
        # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜æ¶ˆè€—
        with torch.no_grad():
            # éå†éªŒè¯æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
            for X_batch, y_batch in val_loader:
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
                outputs = model(X_batch)
                # é€šè¿‡è·å–æ¯ä¸ªè¾“å‡ºè¡Œä¸­æœ€å¤§å€¼çš„ç´¢å¼•æ¥ç¡®å®šé¢„æµ‹æ ‡ç­¾
                preds = torch.argmax(outputs, dim=1)
                # æ‰“å°ç¬¬ä¸€ä¸ª batch çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹å€¼
                # print("çœŸå®æ ‡ç­¾:", y_batch.cpu().numpy())
                # print("é¢„æµ‹æ ‡ç­¾:", preds.cpu().numpy())
                # å°†çœŸå®æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_trueåˆ—è¡¨ä¸­
                y_true.extend(y_batch.cpu().numpy())
                # å°†é¢„æµ‹æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_predåˆ—è¡¨ä¸­
                y_pred.extend(preds.cpu().numpy())

        # çœ‹å“ªäº›ç±»åˆ«çš„ precision/recall å¾ˆä½ï¼› çœ‹çœ‹æ˜¯å¦æ˜¯å­¦ä¸ä¼šæŸä¸€ç±»è¿˜æ˜¯éƒ½å­¦ä¸ä¼š
        print(classification_report(y_true, y_pred))
        # è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡
        acc = accuracy_score(y_true, y_pred)
        # æ‰“å°å½“å‰è½®æ¬¡çš„è®­ç»ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬è½®æ¬¡ã€æŸå¤±å€¼å’ŒéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss:.4f}, Val Acc: {acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_torch_model.pth'))
            print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼Œåªä¿ç•™æœ€æ–°ä¸€è¡Œï¼‰
            with open(log_file_path, 'w') as log_file:
                log_file.write(f"Best Accuracy: {best_acc:.4f}\n")

        writer.add_scalar('Loss/train', total_loss, epoch)
        writer.add_scalar('Accuracy/val', acc, epoch)

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return model


def tradition_model():
    '''
    ä¸ºäº†åˆ¤æ–­æ˜¯å¦æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿæµ‹è¯•ä¸€ä¸ªä¼ ç»Ÿåˆ†ç±»å™¨ï¼ˆå¦‚ RandomForestClassifierï¼‰ï¼š
    å¦‚æœä¼ ç»Ÿæ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œåˆ™è¯´æ˜æ•°æ®æœ¬èº«æ˜¯å¯å­¦çš„ï¼Œé—®é¢˜å‡ºåœ¨ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®¾è®¡æˆ–è®­ç»ƒä¸Šã€‚
    :return:
    '''

    file_path = os.path.join(data_dir, 'train.csv')
    df = pd.read_csv(file_path)
    df = add_agricultural_features(df)
    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name', 'id'])
    y = df['Fertilizer Name'].values
    print(df.describe())

    # è‡ªåŠ¨è¯†åˆ«æ•°å€¼å‹å’Œç±»åˆ«å‹åˆ—
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
    numerical_cols = X.select_dtypes(include=['number']).columns.tolist()

    # å¯é€‰ï¼šæ’é™¤æŸäº›ç‰¹å®šåˆ—ï¼ˆå¦‚'id'ï¼‰
    exclude_cols = ['id']  # å¦‚æœæœ‰éœ€è¦æ’é™¤çš„åˆ—ååˆ—è¡¨
    categorical_cols = [col for col in categorical_cols if col not in exclude_cols]
    numerical_cols = [col for col in numerical_cols if col not in exclude_cols]

    # å®šä¹‰é¢„å¤„ç†å™¨ï¼šç±»åˆ«å‹åˆ—åš OneHotï¼Œæ•°å€¼å‹åˆ—æ ‡å‡†åŒ–

    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
        ('num', StandardScaler(), numerical_cols)
    ])

    # ç¼–ç æ ‡ç­¾ï¼ˆè™½ç„¶ä½ å·²å¤„ç†è¿‡ï¼Œä½†ç¡®ä¿æ˜¯æ•´æ•°å½¢å¼ï¼‰
    le = LabelEncoder()
    y = le.fit_transform(y)

    X_processed = preprocessor.fit_transform(X)


    # âœ… ä¿å­˜å·²ç» fit å¥½çš„ preprocessor
    joblib.dump(preprocessor, os.path.join(current_dir, 'scaler.pkl'))
    print("âœ… ColumnTransformer å·²ä¿å­˜ä¸º scaler.pkl")

    # ä¿å­˜ LabelEncoder
    joblib.dump(le, os.path.join(current_dir, 'label_encoder.pkl'))
    print("âœ… LabelEncoder å·²ä¿å­˜")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)



    # XGBoost ç¤ºä¾‹
    # åˆå§‹åŒ–XGBClassifieræ¨¡å‹ï¼Œé…ç½®ç‰¹å®šçš„å‚æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
    print("XGBoost æ¨¡å‹è®­ç»ƒä¸­...")
    # æ¨¡å‹ä¿å­˜è·¯å¾„
    model_path = "xgboost_model.json"

    # åˆ¤æ–­æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(model_path):
        print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
        model = XGBClassifier()
        model.load_model(model_path)
    else:
        print("ğŸ†• æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œæ­£åœ¨åˆ›å»ºæ–°æ¨¡å‹...")
        model = XGBClassifier(
            n_estimators=500,  # è®¾ç½®æ ‘çš„æ•°é‡ï¼Œå¢åŠ æ•°é‡å¯ä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§
            learning_rate=0.1,  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¯æ£µæ ‘å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            max_depth=6,  # æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œå¢åŠ æ·±åº¦å¯ä»¥æé«˜æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œä½†ä¹Ÿå¯èƒ½å¼•èµ·è¿‡æ‹Ÿåˆ
            min_child_weight=3,  # å¶å­èŠ‚ç‚¹ä¸­æœ€å°çš„æ ·æœ¬æƒé‡å’Œï¼Œç”¨äºæ§åˆ¶è¿‡æ‹Ÿåˆ
            gamma=0.2,  # åœ¨èŠ‚ç‚¹åˆ†è£‚æ—¶çš„æœ€å°å‡å°‘è¯¯å·®é‡ï¼Œå€¼è¶Šå¤§ï¼Œæ¨¡å‹è¶Šä¿å®ˆ
            subsample=0.7,  # è®­ç»ƒæ¯æ£µæ ‘æ—¶ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ï¼Œå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            colsample_bytree=0.6,  # è®­ç»ƒæ¯æ£µæ ‘æ—¶ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
            eval_metric='mlogloss',  # è¯„ä¼°æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿™é‡Œä½¿ç”¨å¤šç±»å¯¹æ•°æŸå¤±
            tree_method='hist'  # ä½¿ç”¨ç›´æ–¹å›¾ç®—æ³•æ¥åŠ é€Ÿæ ‘çš„æ„å»º
        )

    # # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆæ ¹æ® y_trainï¼‰
    # sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)
    #
    # # è½¬æ¢ä¸º numpy æ•°ç»„ä»¥ç¡®ä¿å…¼å®¹æ€§
    # sample_weights = np.array(sample_weights, dtype=np.float32)

    # ä½¿ç”¨æ›´ç²¾ç»†çš„ç±»åˆ«æƒé‡ï¼ˆä¾‹å¦‚æ‰‹åŠ¨æŒ‡å®šæŸç±»æ›´é«˜æƒé‡ï¼‰
    class_weights = {
        0: 0.9,
        1: 0.9,
        2: 0.95,
        3: 1.0,
        4: 1.0,
        5: 1.1,  # å¯¹äºå®¹æ˜“è¢«è¯¯åˆ¤çš„ç±»åˆ«æé«˜æƒé‡
        6: 1.125
    }
    # æ ¹æ® y_train æ„å»º sample_weight æ•°ç»„
    sample_weights = np.array([class_weights[y] for y in y_train], dtype=np.float32)

    model.fit(
        X_train, y_train,
        sample_weight=sample_weights,  # ä½¿ç”¨è®¡ç®—å‡ºçš„æ ·æœ¬æƒé‡
        eval_set=[(X_val, y_val)],
        verbose=100 #æ¯éš”å¤šå°‘ä¸ª epoch æ‰“å°ä¸€æ¬¡è®­ç»ƒæ—¥å¿—ä¿¡æ¯
    )
    model.save_model('xgboost_model.json')
    print("XGBoostè®­ç»ƒç»“æŸ")
    y_pred = model.predict(X_val)
    print("Val Accuracy:", accuracy_score(y_val, y_pred))
    print(classification_report(y_val, y_pred))
    import seaborn as sns

    # æŸ¥çœ‹æ··æ·†çŸ©é˜µ
    sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    # ä¿å­˜å›¾åƒåˆ°æœ¬åœ°
    output_path = os.path.join(current_dir, "image\\é¢„æµ‹æ··æ·†çŸ©é˜µ.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight')  # é«˜æ¸…ä¿å­˜
    print(f"âœ… æ··æ·†çŸ©é˜µå›¾åƒå·²ä¿å­˜è‡³ï¼š{output_path}")
    plt.show()

    print("å¼€å§‹shapåˆ†æ")
    # å¯¼å…¥ SHAP åº“ï¼Œç”¨äºè§£é‡Šæ¨¡å‹é¢„æµ‹ç»“æœ
    import shap
    # åˆ›å»º SHAP è§£é‡Šå™¨ï¼ŒåŸºäºè®­ç»ƒå¥½çš„ XGBoost æ¨¡å‹
    explainer = shap.Explainer(model)

    # è·å–ç‰¹å¾åï¼ˆOneHot å¤„ç†åï¼‰
    ohe = preprocessor.named_transformers_['cat']
    encoded_cat_cols = ohe.get_feature_names_out(categorical_cols)
    all_feature_names = list(encoded_cat_cols) + numerical_cols

    print("ç‰¹å¾æ•°é‡:", X_val.shape[1])
    print("ç‰¹å¾å:", all_feature_names)

    # è®¡ç®—éªŒè¯é›†ä¸Šæ¯ä¸ªæ ·æœ¬çš„ SHAP å€¼ï¼ˆå³æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„å½±å“ï¼‰
    shap_values = explainer(X_val)

    # ç»˜åˆ¶ç¬¬ 5 ç±»å’Œç¬¬ 6 ç±»çš„ SHAP ç‰¹å¾å½±å“å›¾
    # æŸ¥çœ‹å“ªäº›ç‰¹å¾å¯¹è¿™ä¸¤ä¸ªå®¹æ˜“æ··æ·†ç±»åˆ«çš„é¢„æµ‹å½±å“æœ€å¤§
    shap.summary_plot(shap_values[5], X_val)
    shap.summary_plot(shap_values[6], X_val)
    print("shapåˆ†æå®Œæ¯•")

    #æå–æ‰€æœ‰è¢«è¯¯åˆ¤ä¸º 5/6 çš„æ ·æœ¬è¿›è¡Œåˆ†æï¼š
    mask = ((y_pred == 5) | (y_pred == 6)) & (y_val != y_pred)
    X_errors = X_val[mask]
    print("é”™è¯¯åˆ†ç±»æ ·æœ¬ï¼š{}",  X_errors.shape[0])


# -----------------------------
# ç‰¹å¾æ„é€ 
# -----------------------------
def add_agricultural_features(df):
    df['Moisture_Squared'] = df['Moisture'] ** 3
    df['Phosphorous_Squared'] = df['Phosphorous'] ** 2
    df['Nitrogen_Squared'] = df['Nitrogen'] ** 2
    df['Temparature_Squared'] = df['Temparature'] ** 2
    df['Humidity_Squared'] = df['Humidity'] ** 2
    df['NPK_Sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']
    df['N_P_Ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-5)
    df['P_K_Ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-5)
    df['Env_Index'] = df['Temparature'] * df['Humidity'] * df['Moisture']
    df['Crop_Soil_Interaction'] = df['Crop Type'] + '_' + df['Soil Type']
    crop_soil_preference = {
        ('Wheat', 'Clay'): 1.2,
        ('Maize', 'Loam'): 1.3,
        ('Millets', 'Sandy'): 1.5,
    }

    df['Crop_Soil_Preference'] = df.apply(
        lambda row: crop_soil_preference.get((row['Crop Type'], row['Soil Type']), 1.0), axis=1)
    df['Weighted_Nitrogen'] = df['Nitrogen'] * df['Crop_Soil_Preference']
    df['Nitrogen_Potassium_Ratio'] = df['Nitrogen'] / (df['Potassium'] + 1e-5)
    df['Phosphorous_Temp_Index'] = df['Phosphorous'] * df['Temparature']
    return df


if __name__ == '__main__':
    tradition_model()
    # trained_model = train_torch_model()
