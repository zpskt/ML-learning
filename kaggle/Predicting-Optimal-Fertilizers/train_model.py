import os
from collections import Counter

import pandas as pd
import torch
import torch.nn as nn
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from torch.utils.tensorboard import SummaryWriter
import tensorboard
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(current_dir, 'data')

writer = SummaryWriter(log_dir=os.path.join(current_dir, 'runs'))

# åœ¨è®­ç»ƒå¼€å§‹å‰å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_file_path = os.path.join(current_dir, 'best_log.txt')
# è®¾ç½®è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ å½“å‰è®¾å¤‡: {device}")


class FertilizerDataset(torch.utils.data.Dataset):
    def __init__(self, features, labels):
        self.X = features
        self.y = labels

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)


class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        super(MLP, self).__init__()
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        return self.net(x)


def prepare_data():
    '''
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    :return:
    '''
    file_path = os.path.join(data_dir, 'train.csv')
    df = pd.read_csv(file_path)

    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name'])
    y = df['Fertilizer Name'].values
    '''
    äººå·¥æ ¡éªŒæ•°æ®
    '''
    # æ£€æŸ¥åˆ†ç±»æ˜¯å¦å‡è¡¡
    print(Counter(y))

    # å®šä¹‰é¢„å¤„ç†å™¨ï¼šç±»åˆ«å‹åˆ—åš OneHotï¼Œæ•°å€¼å‹åˆ—æ ‡å‡†åŒ–
    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['Soil Type', 'Crop Type']),
        ('num', StandardScaler(), X.columns.difference(['Soil Type', 'Crop Type']))
    ])
    # todo è¿™é‡Œçš„åˆ—å˜å¾—éå¸¸å¤šï¼Œéœ€è¦æ”¹
    X_processed = preprocessor.fit_transform(X)

    # ç¼–ç æ ‡ç­¾ï¼ˆè™½ç„¶ä½ å·²å¤„ç†è¿‡ï¼Œä½†ç¡®ä¿æ˜¯æ•´æ•°å½¢å¼ï¼‰
    le = LabelEncoder()
    y = le.fit_transform(y)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)

    # æ„å»º Dataset å’Œ DataLoader
    train_dataset = FertilizerDataset(X_train, y_train)
    val_dataset = FertilizerDataset(X_val, y_val)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)

    return train_loader, val_loader, len(le.classes_)


def train_torch_model():
    train_loader, val_loader, num_classes = prepare_data()
    input_dim = train_loader.dataset.X.shape[1]  # è‡ªåŠ¨è·å–ç‰¹å¾æ•°é‡
    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´ input_dim
    # æ‰¾ä¸€ä¸ªé€‚åˆå¤šç±»åˆ«åˆ†ç±»çš„æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    # å®šä¹‰ä¼˜åŒ–å™¨
    # optimizer = optim.Adam(model.parameters(), lr=0.0005)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)
    # è®­ç»ƒå¾ªç¯
    epochs = 50
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch")
    # åˆå§‹åŒ–æœ€ä½³å‡†ç¡®ç‡
    best_acc = 0.0

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)

    for epoch in range(epochs):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä»¥ä¾¿å¯ç”¨dropoutã€batch normalizationç­‰åœ¨è®­ç»ƒæ—¶éœ€è¦çš„ç‰¹æ€§
        model.train()
        # åˆå§‹åŒ–æ€»æŸå¤±ä¸º0ï¼Œç”¨äºç´¯è®¡æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±å€¼
        total_loss = 0
        # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œè·å–æ¯ä¸ªæ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾
        for X_batch, y_batch in train_loader:
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆå¦‚GPUï¼‰ä¸Š
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            # æ¸…é›¶ä¼˜åŒ–å™¨çš„æ¢¯åº¦ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯å½±å“ä¸‹ä¸€æ¬¡è®¡ç®—
            optimizer.zero_grad()
            # å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œè·å–æ¨¡å‹çš„è¾“å‡ºç»“æœ
            outputs = model(X_batch)
            # è®¡ç®—æ¨¡å‹è¾“å‡ºç»“æœä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„æŸå¤±å€¼
            loss = criterion(outputs, y_batch)
            # å¯¹æŸå¤±å€¼è¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¨¡å‹å‚æ•°çš„æ¢¯åº¦
            loss.backward()
            # æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¸€æ¬¡æ›´æ–°
            optimizer.step()
            # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„æŸå¤±å€¼åˆ°æ€»æŸå¤±ä¸­ï¼Œç”¨äºåç»­çš„æŸå¤±å€¼è·Ÿè¸ªå’Œå¯è§†åŒ–
            total_loss += loss.item()

        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä»¥ç¦ç”¨dropoutç­‰ä»…åœ¨è®­ç»ƒæœŸé—´å¯ç”¨çš„åŠŸèƒ½
        model.eval()
        # åˆå§‹åŒ–åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        y_true, y_pred = [], []
        # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜æ¶ˆè€—
        with torch.no_grad():
            # éå†éªŒè¯æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
            for X_batch, y_batch in val_loader:
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
                outputs = model(X_batch)
                # é€šè¿‡è·å–æ¯ä¸ªè¾“å‡ºè¡Œä¸­æœ€å¤§å€¼çš„ç´¢å¼•æ¥ç¡®å®šé¢„æµ‹æ ‡ç­¾
                preds = torch.argmax(outputs, dim=1)
                # å°†çœŸå®æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_trueåˆ—è¡¨ä¸­
                y_true.extend(y_batch.cpu().numpy())
                # å°†é¢„æµ‹æ ‡ç­¾ä»Tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå¹¶æ·»åŠ åˆ°y_predåˆ—è¡¨ä¸­
                y_pred.extend(preds.cpu().numpy())

        # çœ‹å“ªäº›ç±»åˆ«çš„ precision/recall å¾ˆä½ï¼› çœ‹çœ‹æ˜¯å¦æ˜¯å­¦ä¸ä¼šæŸä¸€ç±»è¿˜æ˜¯éƒ½å­¦ä¸ä¼š
        print(classification_report(y_true, y_pred))
        # è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡
        acc = accuracy_score(y_true, y_pred)
        # æ‰“å°å½“å‰è½®æ¬¡çš„è®­ç»ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬è½®æ¬¡ã€æŸå¤±å€¼å’ŒéªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss:.4f}, Val Acc: {acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_torch_model.pth'))
            print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼Œåªä¿ç•™æœ€æ–°ä¸€è¡Œï¼‰
            with open(log_file_path, 'w') as log_file:
                log_file.write(f"Best Accuracy: {best_acc:.4f}\n")

        scheduler.step(total_loss)
        writer.add_scalar('Loss/train', total_loss, epoch)
        writer.add_scalar('Accuracy/val', acc, epoch)

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return model


if __name__ == '__main__':
    trained_model = train_torch_model()
