import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score
import os
import joblib

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(current_dir, 'data')

# è®¾ç½®è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ å½“å‰è®¾å¤‡: {device}")


class FertilizerDataset(torch.utils.data.Dataset):
    def __init__(self, features, labels):
        self.X = features
        self.y = labels

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)


class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        super(MLP, self).__init__()
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            # è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢
            nn.Linear(input_dim, hidden_dim),
            # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§
            nn.ReLU(),
            # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸¢å¼ƒç‡0.3
            nn.Dropout(0.3),
            # ç¬¬ä¸€ä¸ªéšè—å±‚åˆ°è‡ªèº«çš„çº¿æ€§å˜æ¢ï¼ŒåŠ æ·±ç½‘ç»œç†è§£
            nn.Linear(hidden_dim, hidden_dim),
            # å†æ¬¡ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
            nn.ReLU(),
            # æœ€åä¸€ä¸ªéšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢
            nn.Linear(hidden_dim, num_classes)
        )


    def forward(self, x):
        return self.net(x)


def prepare_data():
    '''
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    :return:
    '''
    # åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
    file_path = os.path.join(data_dir, 'processed_train.csv')
    df = pd.read_csv(file_path)

    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name']).values
    y = df['Fertilizer Name'].values

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # ç¼–ç æ ‡ç­¾
    le = LabelEncoder()
    y = le.fit_transform(y)

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # æ„å»º Dataset å’Œ DataLoader
    train_dataset = FertilizerDataset(X_train, y_train)
    val_dataset = FertilizerDataset(X_val, y_val)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)

    return train_loader, val_loader, len(le.classes_)


def train_torch_model():
    train_loader, val_loader, num_classes = prepare_data()
    input_dim = train_loader.dataset.X.shape[1]  # è‡ªåŠ¨è·å–ç‰¹å¾æ•°é‡
    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´ input_dim
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # è®­ç»ƒå¾ªç¯
    epochs = 50
    best_acc = 0.0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # éªŒè¯
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                preds = torch.argmax(outputs, dim=1)

                y_true.extend(y_batch.cpu().numpy())
                y_pred.extend(preds.cpu().numpy())

        acc = accuracy_score(y_true, y_pred)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Val Acc: {acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_torch_model.pth'))
            print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return model


if __name__ == '__main__':
    trained_model = train_torch_model()
