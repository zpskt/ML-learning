import os
from collections import Counter

import joblib
import numpy as np
import pandas as pd
import torch
from lightgbm import LGBMClassifier
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, TargetEncoder
from sklearn.preprocessing import StandardScaler
from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from xgboost import XGBClassifier

from data_preprocessing import save_processed_data

le = LabelEncoder()

def add_agricultural_features(df):
    '''
    æ„é€ å†œä¸šé¢†åŸŸç‰¹å¾
    :param df:
    :return:
    '''
    # NPK æ€»å’Œ
    df['NPK_Sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']

    # N/P æ¯”ä¾‹
    df['N_P_Ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-5)

    # P/K æ¯”ä¾‹
    df['P_K_Ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-5)

    # ç¯å¢ƒé€‚å®œæ€§æŒ‡æ ‡ï¼ˆæ¸©åº¦ Ã— æ¹¿åº¦ Ã— æ°´åˆ†ï¼‰
    df['Env_Index'] = df['Temparature'] * df['Humidity'] * df['Moisture']

    # è‚¥åŠ›ç»¼åˆè¯„åˆ†ï¼ˆå‡è®¾ N=0.3, P=0.3, K=0.4 æƒé‡ï¼‰
    df['Fertility_Score'] = (
            df['Nitrogen'] * 0.3 +
            df['Phosphorous'] * 0.3 +
            df['Potassium'] * 0.4
    )

    # å¢åŠ ä½œç‰©å¯¹è¥å…»å…ƒç´ çš„åå¥½ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰
    crop_n_preference = {
        'Wheat': 0.8,
        'Maize': 0.7,
        'Oil seeds': 0.3,
        'Paddy': 0.5,
        'Cotton': 0.6,
        'Barley': 0.7,
        'Millets': 0.5,
        'Sugarcane': 0.4,
        'Ground Nuts': 0.4,
        'Tobacco': 0.5,
        'Pulses': 0.4
    }

    df['Crop_Nitrogen_Preference'] = df['Crop Type'].map(crop_n_preference).fillna(0.5)
    df['Weighted_N'] = df['Nitrogen'] * df['Crop_Nitrogen_Preference']

    return df

# -----------------------------
# PyTorch Autoencoder
# -----------------------------
class Autoencoder(nn.Module):
    def __init__(self, input_dim, encoding_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, encoding_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return z, x_recon

# å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„
class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        super(MLP, self).__init__()
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)


# -----------------------------
# è®­ç»ƒå‡½æ•°
# -----------------------------
def train_mlp_model(model, criterion, optimizer, dataloader, epochs=50):
    model.train()
    for epoch in range(epochs):
        print(f" MLP Epoch {epoch+1}/{epochs}")
        for x_batch, y_batch in dataloader:
            out = model(x_batch)
            loss = criterion(out, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()


def train_autoencoder(autoencoder, criterion_ae, optimizer_ae, dataloader, epochs=50):
    autoencoder.train()
    for epoch in range(epochs):
        for x_batch, _ in dataloader:
            _, recon = autoencoder(x_batch)
            loss = criterion_ae(recon, x_batch)
            optimizer_ae.zero_grad()
            loss.backward()
            optimizer_ae.step()

def load_data_and_preprocess(data='train.csv'):
    '''
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    :return:
    '''
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, 'data')

    print("å¼€å§‹åŠ è½½æ•°æ®å¹¶å¤„ç†")

    # -----------------------------
    # 1. åŠ è½½æ•°æ®
    # -----------------------------
    file_path = os.path.join(data_dir, data)
    df = pd.read_csv(file_path)

    # -----------------------------
    # 3. æ„é€ å†œä¸šé¢†åŸŸç‰¹å¾
    # -----------------------------
    # åº”ç”¨ç‰¹å¾æ„é€ 
    df = add_agricultural_features(df)

    # -----------------------------
    # 4. ç‰¹å¾ç¼–ç å¤„ç†
    # -----------------------------
    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=['Fertilizer Name', 'id'])
    y = df['Fertilizer Name'].values

    # âœ… æ·»åŠ è¿™ä¸€æ®µæ¥å¯¹ y è¿›è¡Œç¼–ç 
    y = le.fit_transform(y)
    print("âœ… ç›®æ ‡å˜é‡å·²ç¼–ç ä¸ºæ•°å€¼ç±»å‹:", dict(enumerate(le.classes_)))

    # ä¿å­˜ LabelEncoder
    joblib.dump(le, os.path.join(current_dir, 'label_encoder.pkl'))
    print("âœ… LabelEncoder å·²ä¿å­˜")

    # ç±»åˆ«å‹åˆ—å’Œæ•°å€¼å‹åˆ—
    categorical_cols = ['Soil Type', 'Crop Type']
    numerical_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']
    # æ‰€æœ‰ç‰¹å¾åˆ—ä¸­ç­›é€‰å‡ºâ€œæ—¢ä¸æ˜¯ç±»åˆ«å‹ä¹Ÿä¸æ˜¯æ•°å€¼å‹â€çš„åˆ—ï¼Œ å³æ„é€ çš„å†œä¸šé¢†åŸŸæ–°ç‰¹å¾ï¼ˆå¦‚ NPK æ¯”ä¾‹ã€ç¯å¢ƒå› å­ç­‰ï¼‰
    '''
    è¿™äº›å†œä¸šç‰¹å¾ä¸ºä»€ä¹ˆè¦å•ç‹¬å¤„ç†ï¼Ÿ
        å› ä¸ºï¼š
        å®ƒä»¬å·²ç»ç»è¿‡äººå·¥æ„é€ ï¼Œå¯èƒ½å…·æœ‰æ›´å¼ºçš„éçº¿æ€§å…³ç³»ï¼›
        å¯èƒ½éœ€è¦ä¸åŸå§‹æ•°å€¼ç‰¹å¾ä¸€èµ·æ ‡å‡†åŒ–ï¼›
        ä¸å±äºåŸå§‹çš„ç±»åˆ«æˆ–æ•°å€¼ç‰¹å¾ï¼Œæ‰€ä»¥è¦å•ç‹¬æå–å‡ºæ¥ç”¨äºåç»­é¢„å¤„ç†ã€‚
    '''
    agri_feature_cols = [col for col in X.columns if col not in categorical_cols + numerical_cols]

    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', TargetEncoder(), categorical_cols),
            ('num', StandardScaler(), numerical_cols + agri_feature_cols)
        ],
        remainder='passthrough'
    )
    X_processed = preprocessor.fit_transform(X, y)
    # âœ… ä¿å­˜å·²ç» fit å¥½çš„ preprocessor
    joblib.dump(preprocessor, os.path.join(current_dir, 'scaler.pkl'))
    print("âœ… ColumnTransformer å·²ä¿å­˜ä¸º scaler.pkl")

    # å°†ç¼–ç åçš„ y æ·»åŠ ä¸ºæ–°åˆ—
    feature_names = preprocessor.get_feature_names_out()
    # è½¬æ¢ä¸º pandas DataFrame
    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)

    # âœ… æ·»åŠ ç›®æ ‡å˜é‡
    # X_processed_df['Encoded_Label'] = y  # ç¼–ç åçš„ yï¼ˆint ç±»å‹ï¼‰

    # å¦‚æœä½ è¿˜æƒ³æ·»åŠ åŸå§‹çš„ Fertilizer Nameï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰ä¹Ÿå¯ä»¥åŠ ä¸Š
    df_original = pd.read_csv(os.path.join(data_dir, 'train.csv'))
    original_labels = df_original['Fertilizer Name'].values
    X_processed_df['Fertilizer Name'] = original_labels  # åŸå§‹å­—ç¬¦ä¸²æ ‡ç­¾ï¼ˆä¾¿äºäººå·¥ç†è§£ï¼‰

    # âœ… æŠŠå½“å‰å¤„ç†åçš„Xå’Œyå­˜å…¥ä¸€ä¸ªæ–°çš„csvä¸­
    save_processed_data(X_processed_df, 'processed_train.csv')
    print("âœ… å·²ä¿å­˜å¤„ç†åçš„æ•°æ®è‡³ processed_train.csv")

    # åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)
    print("âœ… æ•°æ®å¤„ç†å®Œæ¯•")
    return X_train, X_val, y_train, y_val



def main():
    print("ğŸš€ å¯åŠ¨ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹è®­ç»ƒæµç¨‹")
    X_train, X_val, y_train, y_val = load_data_and_preprocess()

    # -----------------------------
    # 5.1 æ¨¡å‹è®­ç»ƒï¼šXGBoost
    # -----------------------------
    print("\nğŸŒ³ æ­£åœ¨è®­ç»ƒ XGBoost æ¨¡å‹...")

    model = XGBClassifier(
        n_estimators=3000,
        learning_rate=0.05,
        max_depth=7,
        subsample=0.7,
        colsample_bytree=0.6,
        eval_metric='mlogloss',
        early_stopping_rounds=20,
        use_label_encoder=False,
        verbosity=0,
        random_state=42
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=100
    )
    print("\nğŸŒ³ è®­ç»ƒ XGBoost æ¨¡å‹å®Œæ¯•")
    # ä¿å­˜æ¨¡å‹
    model.save_model('xgboost_model.json')
    # -----------------------------
    # 5.2 æ¨¡å‹è®­ç»ƒï¼šLightGbm
    # -----------------------------
    # print("\nğŸ’¡ æ­£åœ¨è®­ç»ƒ LightGBM æ¨¡å‹...")
    #
    # lgb_model = LGBMClassifier(
    #     n_estimators=3000,
    #     learning_rate=0.1,
    #     max_depth=7,
    #     num_leaves=50,
    #     subsample=0.7,
    #     colsample_bytree=0.6,
    #     objective='multiclass',
    #     num_class=len(le.classes_),  # è‡ªåŠ¨è¯†åˆ«ç±»åˆ«æ•°
    #     eval_metric='multi_logloss',
    #     early_stopping_rounds=20,
    #     verbose=-1,
    #     random_state=42
    # )
    #
    # lgb_model.fit(
    #     X_train, y_train,
    #     eval_set=[(X_val, y_val)]
    # )
    # print("\nğŸ’¡ è®­ç»ƒ LightGBM æ¨¡å‹å®Œæ¯•")

    # -----------------------------
    # æ¨¡å‹è®­ç»ƒï¼šPyTorch MLP
    # -----------------------------
    # print("\nğŸ§  æ­£åœ¨è®­ç»ƒ PyTorch ç¥ç»ç½‘ç»œæ¨¡å‹...")
    #
    # X_train_torch = torch.tensor(X_train, dtype=torch.float32)
    # X_val_torch = torch.tensor(X_val, dtype=torch.float32)
    # y_train_torch = torch.tensor(y_train, dtype=torch.long)
    # y_val_torch = torch.tensor(y_val, dtype=torch.long)
    #
    # dataset = TensorDataset(X_train_torch, y_train_torch)
    # loader = DataLoader(dataset, batch_size=32, shuffle=True)
    #
    # mlp = MLP(X_train.shape[1], len(le.classes_))
    # criterion = nn.CrossEntropyLoss()
    # optimizer = optim.Adam(mlp.parameters(), lr=0.001)

    # train_mlp_model(mlp, criterion, optimizer, loader, epochs=50)

    # -----------------------------
    # æ¨¡å‹è®­ç»ƒï¼šPyTorch Autoencoder
    # -----------------------------
    # print("\nğŸ” æ­£åœ¨è®­ç»ƒ PyTorch è‡ªåŠ¨ç¼–ç å™¨...")
    #
    # ae_dataset = TensorDataset(X_train_torch, X_train_torch)
    # ae_loader = DataLoader(ae_dataset, batch_size=32, shuffle=True)
    #
    # ae = Autoencoder(X_train.shape[1], 32)
    # criterion_ae = nn.MSELoss()
    # optimizer_ae = optim.Adam(ae.parameters(), lr=0.001)
    #
    # train_autoencoder(ae, criterion_ae, optimizer_ae, ae_loader, epochs=50)
    #
    # with torch.no_grad():
    #     X_train_encoded = ae.encoder(X_train_torch).numpy()
    #     X_val_encoded = ae.encoder(X_val_torch).numpy()
    #
    # ae_classifier = LogisticRegression()
    # ae_classifier.fit(X_train_encoded, y_train)

    # -----------------------------
    # å¤šæ¨¡å‹é›†æˆé¢„æµ‹
    # -----------------------------
    # print("\nğŸ“‹ å¤šæ¨¡å‹è¯„ä¼°ä¸­...")
    #
    # xgb_proba = model.predict_proba(X_val)
    # # lgb_proba = lgb_model.predict_proba(X_val)
    # # mlp_proba = mlp(X_val_torch).softmax(dim=1).detach().numpy()
    # ae_proba = ae_classifier.predict_proba(X_val_encoded)
    #
    # avg_proba = (xgb_proba + ae_proba) / 4
    # ensemble_pred = np.argmax(avg_proba, axis=1)
    #
    # acc = accuracy_score(y_val, ensemble_pred)
    # print(f"\nâœ… é›†æˆæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {acc:.4f}")
    # print("\nğŸ“‹ é›†æˆæ¨¡å‹åˆ†ç±»æŠ¥å‘Š:")
    # print(classification_report(y_val, ensemble_pred))


    # -----------------------------
    # 6.1 å•ä¸€ç»“æœæ¨¡å‹è¯„ä¼°
    # -----------------------------
    print("\nğŸ“‹ å•ä¸€ç»“æœæ¨¡å‹è¯„ä¼°ä¸­...")
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)

    print(f"\nâœ… éªŒè¯é›†å‡†ç¡®ç‡: {acc:.4f}")
    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_val, y_pred))

    # -----------------------------
    # 6.2 å¤šç»“æœæ¨¡å‹è¯„ä¼°
    # -----------------------------
    print("\nğŸ“‹ å¤šç»“æœæ¨¡å‹è¯„ä¼°ä¸­...")
    y_proba = model.predict_proba(X_val)
    # è·å– top-3 çš„é¢„æµ‹ç±»åˆ«ç´¢å¼•
    top_3_indices = np.argsort(y_proba, axis=1)[:, -3:]
    y_val_flat = y_val.ravel()
    # åˆ¤æ–­çœŸå®æ ‡ç­¾æ˜¯å¦åœ¨ top-3 é¢„æµ‹ä¸­
    top_3_correct = np.sum([y_val_flat[i] in top_3_indices[i] for i in range(len(y_val_flat))])
    # è®¡ç®— Top-3 å‡†ç¡®ç‡
    top_3_accuracy = top_3_correct / len(y_val_flat)
    print(f"\nâœ… éªŒè¯é›† Top-3 å‡†ç¡®ç‡: {top_3_accuracy:.4f}")


def generate_submission(file_name='test.csv', output_file='submission.csv', top_k=3):
    '''
       åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
       :return:
       '''
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, 'data')
    print("å¼€å§‹åŠ è½½æ•°æ®å¹¶å¤„ç†")

    # -----------------------------
    # 1. åŠ è½½æ•°æ®
    # -----------------------------
    file_path = os.path.join(data_dir, file_name)
    df = pd.read_csv(file_path)

    # æå– id åˆ—
    ids = df['id'].values

    # -----------------------------
    # 3. æ„é€ å†œä¸šé¢†åŸŸç‰¹å¾
    # -----------------------------
    # åº”ç”¨ç‰¹å¾æ„é€ 
    df = add_agricultural_features(df)

    # -----------------------------
    # 4. ç‰¹å¾ç¼–ç å¤„ç†
    # -----------------------------
    # ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(columns=[ 'id'])

    # ğŸ“Œ å‡è®¾ä½ å·²ç»ä¿å­˜äº† preprocessorï¼š
    import joblib
    preprocessor = joblib.load(os.path.join(current_dir, 'scaler.pkl'))  # ç¤ºä¾‹è·¯å¾„ï¼Œè¯·æ›¿æ¢ä¸ºä½ å®é™…ä¿å­˜çš„ ColumnTransformer

    # åº”ç”¨å˜æ¢ï¼ˆä¸è¿›è¡Œ fitï¼‰
    X_processed = preprocessor.transform(X)

    """
    å¯¹ test.csv ä¸­çš„æ¯ä¸ªæ ·æœ¬é¢„æµ‹ Top-K æ¨èï¼Œå¹¶ä¿å­˜ä¸º submission.csv
    """
    print("ğŸ“„ å¼€å§‹ç”Ÿæˆæäº¤æ–‡ä»¶...")
    # åŠ è½½æ¨¡å‹jsonæ–‡ä»¶
    model = XGBClassifier()
    model.load_model("xgboost_model.json")

    probas = model.predict_proba(X_processed)


    # è·å– Top-K ç±»åˆ«ç´¢å¼•
    top_k_indices = np.argsort(probas, axis=1)[:, -top_k:]
    # åŠ è½½ LabelEncoder
    le = joblib.load(os.path.join(current_dir, 'label_encoder.pkl'))
    # è½¬æ¢ä¸ºåŸå§‹ç±»åˆ«åç§°ï¼ˆå‡è®¾ le å·²ä»è®­ç»ƒé˜¶æ®µåŠ è½½ï¼‰
    predicted_labels = np.array([le.inverse_transform(row) for row in top_k_indices])

    # ç”Ÿæˆæäº¤æ ¼å¼å­—ç¬¦ä¸²ï¼šå¤šä¸ªè‚¥æ–™åç§°ç”¨ç©ºæ ¼åˆ†éš”
    predicted_strings = [" ".join(row) for row in predicted_labels]

    # åˆ›å»ºæäº¤ DataFrame
    submission_df = pd.DataFrame({
        'id': ids,
        'Fertilizer Name': predicted_strings
    })

    # ä¿å­˜ä¸º CSV æ–‡ä»¶
    submission_df.to_csv(output_file, index=False)
    print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜è‡³ {output_file}")


if __name__ == "__main__":
    # main()
    generate_submission()
